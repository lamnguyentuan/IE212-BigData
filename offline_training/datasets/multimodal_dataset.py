"""
Multimodal Dataset Class.

Loads features from .npz files generated by the preprocessing pipeline.
"""

import torch
import numpy as np
from torch.utils.data import Dataset
from pathlib import Path
from typing import Optional, Tuple

class MultimodalDataset(Dataset):
    def __init__(self, npz_path: str, split: str = "train", split_ratio: float = 0.9):
        """
        Args:
            npz_path: Path to the .npz file containing the dataset.
            split: "train" or "val".
            split_ratio: Ratio of training data.
        """
        self.npz_path = Path(npz_path)
        if not self.npz_path.exists():
            raise FileNotFoundError(f"Dataset not found at {self.npz_path}")
            
        # Load logic similar to what FeatureSaver wrote
        # keys: video_ids, video_embs, audio_embs, text_embs, metadata_numeric, labels
        data = np.load(self.npz_path, allow_pickle=True)
        
        self.video_embs = data["video_embs"]
        self.audio_embs = data["audio_embs"]
        self.text_embs = data["text_embs"]
        self.metadata_numeric = data["metadata_numeric"]
        self.labels = data["labels"]
        self.video_ids = data["video_ids"]
        
        # Simple split logic based on index
        # For production, utilize a fixed split file or 'split' column if available/saved
        total_len = len(self.labels)
        split_idx = int(total_len * split_ratio)
        
        indices = np.arange(total_len)
        # Seed for reproducibility of split if shuffle not done externally
        # Ideally we shuffle once before saving or use fixed seed
        np.random.seed(42)
        np.random.shuffle(indices)
        
        if split == "train":
            self.indices = indices[:split_idx]
        else:
            self.indices = indices[split_idx:]
            
    def __len__(self) -> int:
        return len(self.indices)

    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, int]:
        real_idx = self.indices[idx]
        
        v = torch.tensor(self.video_embs[real_idx], dtype=torch.float32)
        a = torch.tensor(self.audio_embs[real_idx], dtype=torch.float32)
        t = torch.tensor(self.text_embs[real_idx], dtype=torch.float32)
        m = torch.tensor(self.metadata_numeric[real_idx], dtype=torch.float32)
        l = int(self.labels[real_idx])
        
        return v, a, t, m, l
